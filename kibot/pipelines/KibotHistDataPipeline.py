import shutil
import os
import numpy as np
import pandas as pd
import re
import dateutil.tz as tz
from enum import Enum
from functools import partialmethod
from datetime import datetime, date
from typing import Tuple
from airflow_pipelines.airflow_pipeline_adapters import AirflowStageMixin
from pipelines.DataPipeline import BaseStage, StageResult, PipelineContext
from storage.file.FileStorage import CSVVectorizedStorage
from commons.ftp.FtpWrapper import FtpWrapper, FtpWrapperParameters
from commons.utils.FileUtils import extract_with_7zip

# The cdf bar schema - autogenerated
from schemas.schemas_py.marketdata.cdf_bar import MarketDataBar

# ----------------------------------------------------------------------------
# ----------------------------------------------------------------------------
# Collection of Kibot dedicate utils
# ----------------------------------------------------------------------------
# ----------------------------------------------------------------------------

class KibotRunType(Enum):
    Equity = 0
    Etf = 1


def extract_and_validate_date(path):
    # Extract the 8-digit pattern before .exe
    match = re.search(r'(\d{8})\.exe$', path)
    if not match:
        return None  # No valid 8-digit sequence found
    date_str = match.group(1)
    # Validate the date format (YYYYMMDD)
    try:
        datetime.strptime(date_str, "%Y%m%d")
        return date_str  # It's a valid date
    except ValueError:
        return None  # Invalid date (e.g., 20230230)


def extract_date(date_str):
    date_str_ = date_str.split('T')[0]
    return datetime.strptime(date_str_,'%Y-%m-%d').date()


def _validate_op(row: list, symbl: str, reportName: str, filter: list, value, op, errors="strict", replace_with=np.nan):
    """
    Check for field <op> value for the fields where filter is True
    row: list of parsed data (the fields)
    filter: list of bools indicating which fields to check
    value: value to compare the field with
    op: operator function of the form op(<field>, <value>) -> bool
    errors: one of "strict" (raise Exception) or "replace" replace with the parameter replace_with
    replace_with: value to replace invalid values, iff errors is "replace"
    """

    # This will raise an exception anst stop the Engne - so we have to update it
    # assert len(row) == len(filter), "filter and row must have the same length"
    if len(row) != len(filter):
        ss = "validate() - Warning on " + symbl + " wrong line - " + str(row)

    assert errors in ["strict", "replace"], "errors must be one of 'strict' or 'replace'"

    result = []
    for i, field in enumerate(row):
        if filter[i] and not op(float(field), value):
            if errors == "strict":
                raise Exception(f"in row: {row}\nvalue {field} is not positive")
            else:
                ss = "validate() - Warning on " + symbl + " line - " + str(row) + " - field: " + str(field)
                result.append(replace_with)
        else:
            result.append(field)

    assert len(result) == len(row)
    return result


def validate(row: list, symbl: str, reportName: str, errors="replace", replace_with=np.nan):
    """
    validate a single row of CDF - Common Data Format
    v2 N.9 - FIELDS - OK
         --------------------------------------------------------------------------------------------
         Date, Time, Open, High, Low, Close, Volume, Vwap, count
         --------------------------------------------------------------------------------------------
    """
    result = _validate_gt(row, symbl, reportName, [False, False, True, True, True, True, False, False, False], 0,
                               errors=errors, replace_with=replace_with)
    result = _validate_ge(result, symbl, reportName,
                               [False, False, False, False, False, False, True, False, False], 0, errors=errors,
                               replace_with=replace_with)
    return result


def strToIso8601TZ(ss : str,HOUR=23, MIN=59, SECS=00):
    # Validate input String
    validateString(ss)
    # convert string to datetime
    dd=datetime.strptime(ss, '%m/%d/%Y')
    # set the Time and ZoneInfo
    dd=dd.replace(hour=HOUR, minute=MIN, second=SECS,tzinfo=tz.gettz('America/New_York'))
    return dd.isoformat()


def validateString(ss : str) :
     try:
         datetime.strptime(ss, '%m/%d/%Y')
     except ValueError:
         raise ValueError("Incorrect data format, should be MM/DD/YYYY passed: "+ ss)


def splitTime(symbl,tt):
    try:
      ttime=tt.split(':')
      if len(ttime)==2 :
         hh=int(ttime[0])
         mm=int(ttime[1])
         return [hh,mm]
      else : raise Exception("Parse::splitTime() - Wrrot to split the Time "+ tt + " Field for "+symbl )
    except Exception as e:
        return [0,0]


def parse(line: str, symb:str)-> list:
    lst=[]
    try:
       if type(line) is bytes:
           ll=line.decode('utf8','ignore').rstrip().split(',')
       else:
           ll=line.rstrip().split(',')
       if len(ll) ==6:  # DAILY UPDATES (not Time)
          dd=strToIso8601TZ(ll[0])
          lst.append(dd)
          lst.append(np.nan)
          lst.append(ll[1])
          lst.append(ll[2])
          lst.append(ll[3])
          lst.append(ll[4])
          lst.append(ll[5])
          lst.append(np.nan)
          lst.append(np.nan)
       elif len(ll) ==7:  # INTRADAY (add Time)
          ## Check the TIme from Kibot
          ttime=splitTime(symb,ll[1])
          hh=ttime[0]
          mm=ttime[1]
          dd =strToIso8601TZ(ll[0],hh,mm)
          lst.append(dd)
          #lst.append(ll[0])
          lst.append(ll[1])
          lst.append(ll[2])
          lst.append(ll[3])
          lst.append(ll[4])
          lst.append(ll[5])
          lst.append(ll[6])
          lst.append(np.nan)
          lst.append(np.nan)
       else: raise  TypeError("Kibot wrong input format: "+ line +" Check che input file")
       return  lst
    except Exception as e:
        print("KiBotParser::parse() Error parsing the line: "+ str(e) +" - Wrong format!")
        lst.append(line)
    return lst


def parseEntryToCDF(file_name, extracted_path) -> list:
    #self._logger.info("PyRarFileManager::parseEntryToCDF(): - Parsing file: "+ name)
    cdf=[]
    base=os.path.basename(file_name)
    path=os.path.dirname(file_name)
    if re.match('[A-Z]*[0-9]*.[A-Z]*.txt',base):
        symb = base.replace('.txt', "")
        f_open = extracted_path  +"/" + base
        with open(f_open) as f:
            cdf=[parse(line,symb) for line in f ]
    else:
             print("PyRarFileManager::parseEntryToCDF(): - Skipped File "+ file_name)
    return cdf, symb

import operator
_validate_ge = partialmethod(_validate_op, op=operator.ge)
_validate_gt = partialmethod(_validate_op, op=operator.gt)


def _create_cdf_file_storage(svectorize_symbol : bool,
                              svectorize_daily  : bool,
                              csv_path : str,
                              csv_prefix : str) -> CSVVectorizedStorage[MarketDataBar]:
        """
        Factory function to create a properly configured storage instance.

        :param svectorize_symbol: Whether to vectorize by symbol
        :param svectorize_daily: Whether to vectorize by date
        :return: Configured CSVVectorizedStorage instance
        """
        vectorize_fields = []
        if svectorize_daily:
            vectorize_fields.append(MarketDataBar.Fields.DATE)
        if svectorize_symbol:
            vectorize_fields.append(MarketDataBar.Fields.SYMB)

        storage = CSVVectorizedStorage(
            schema_class=MarketDataBar,
            vectorize_by=vectorize_fields,
            csv_path=csv_path,
            csv_filename_prefix=csv_prefix)
        return storage


def _parsing_and_writing_cdf(filesToParse : str,
                              workingpath : str,
                              processed_cdf_path : str,
                              is_daily : bool,
                              yyyymmdd : int,
                              processed_earliest_date : date):
    '''
    parse an extracted kibot to svectorized Cdfs
    :param fileToParse:
    :param workingpath:
    :param processed_cdf_path:
    :param is_daily:
    :param yyyymmdd:
    :return:
    '''
    count = 0
    symbols_to_dates = []

    cdf_storage = _create_cdf_file_storage(svectorize_symbol=True,
                                            svectorize_daily =True,
                                            csv_path=processed_cdf_path,
                                            csv_prefix='')

    for nf in filesToParse:
        try:
            cdf, symb = parseEntryToCDF(nf, workingpath)
            if len(cdf) != 0:
                df = pd.DataFrame(cdf, columns=[MarketDataBar.Fields.DATE,
                                                MarketDataBar.Fields.DATETIME,
                                                MarketDataBar.Fields.OP,
                                                MarketDataBar.Fields.HI,
                                                MarketDataBar.Fields.LO,
                                                MarketDataBar.Fields.CL,
                                                MarketDataBar.Fields.VLM,
                                                MarketDataBar.Fields.VWAP,
                                                MarketDataBar.Fields.COUNT])

                df[MarketDataBar.Fields.DATETIME] = pd.to_datetime(df[MarketDataBar.Fields.DATETIME])
                df[MarketDataBar.Fields.DATE] = [extract_date(t) for t in df[MarketDataBar.Fields.DATE]]
                #
                if len(df) > 1:
                    nice = df[MarketDataBar.Fields.DATE] >= processed_earliest_date
                    df = df[nice]

                df[MarketDataBar.Fields.SYMB] = symb

                # Write the Cdf files
                cdf_storage.write(df)
                count += 1

                # The symbols to dates written summary
                symbols_to_dates.append({
                    'symb': symb,
                    'dates': len(df[MarketDataBar.Fields.DATE].unique()),
                    'start': df[MarketDataBar.Fields.DATE].min()
                })

        except Exception as e:
            print("Error processing: " + nf)

    processing_summary_file = processed_cdf_path + '/processing_summary_' + str(yyyymmdd) + '.csv'
    pd.DataFrame(symbols_to_dates).to_csv(processing_summary_file)
    return count


# ----------------------------------------------------------------------------
# ----------------------------------------------------------------------------
# Kibot Stages
# ----------------------------------------------------------------------------
# ----------------------------------------------------------------------------


class KibotDailyFtpDump(BaseStage, AirflowStageMixin):
    """
        Downloads daily Kibot dumps from FTP and validates their presence.
        Can also be used as an AirflowStageMixin.
    """
    kibot_ftp_section = 'Kibot FTP'


    def run(self, ctx: PipelineContext) -> Tuple[StageResult, str]:
        errors : str = None
        try:
            outfolder  = ctx.params.get(self.kibot_ftp_section, 'outfolder')
            ftp_params = FtpWrapperParameters(config_parser=ctx.params,
                                              ftp_section=self.kibot_ftp_section)
            ftp_worker = FtpWrapper(ftp_params,
                                    ctx.logger)
            os.makedirs(outfolder, exist_ok=True)

        except Exception as e:
            errors = 'Error while running KibotDailyFtpDump ' + self.name + ': ' + str(e)
            ctx.logger.error(errors)

        # Verify if the data was dumped
        yyyymmdd = ctx.params.config_parser.getstr(self.kibot_ftp_section, "yyyymmdd")
        etf_daily_file = outfolder + '/All\ ETFs/Daily/' + str(yyyymmdd) +'.exe'
        stk_daily_file = outfolder + '/All\ Stocks/Daily/' + str(yyyymmdd) + '.exe'
        etf_daily_file_exists = os.path.exists(etf_daily_file)
        message = 'Etf file downloaded: ' + str(etf_daily_file_exists)
        stk_daily_file_exists = os.path.exists(stk_daily_file)
        message += '; Stock file downloaded: ' + str(stk_daily_file_exists)
        if stk_daily_file_exists and etf_daily_file_exists:
            if errors == '':
                stage = StageResult.SUCCESS
            else:
                stage = StageResult.SUCCESS_WITH_WARNINGS
        else:
            stage = StageResult.FAIL
        final_msg = message + '. errors: ' + str(errors)
        return (stage, final_msg)



class KibotCdfProcessor(BaseStage, AirflowStageMixin):

    kibot_process_section = 'Kibot Cdf Processing'

    process_earliest_daily  : date = datetime.strptime('2016-01-01', '%Y-%m-%d').date()
    process_earliest_intra  : date = datetime.strptime('2019-01-01', '%Y-%m-%d').date()

    def _process_kibot_archive(self, archive_in: str,
                                     working_path: str,
                                     processed_cdf_path : str,
                                     daily : bool) -> (bool, str):
        yyyymmdd = extract_and_validate_date(archive_in)
        if yyyymmdd is None:
            return (False, 'The archive in could not be parsed to a valid date: ' + archive_in)
        try:
            # Create Historical Local Directory
            if not os.path.exists(working_path): os.makedirs(working_path)
            success = False
            try:
                extract_with_7zip(archive_in, working_path)
                outpath = processed_cdf_path + '/' + str(yyyymmdd)
                os.makedirs(outpath)
                # Parse the daily
                count = 0
                if daily:
                    fileToParse_Daily = os.listdir(working_path + "/Daily")
                    count = _parsing_and_writing_cdf(fileToParse_Daily,
                                                       working_path + "/Daily",
                                                       outpath,
                                                       is_daily=True,
                                                       yyyyymmdd=yyyymmdd,
                                                       processed_earliest_date=self.process_earliest_daily)

                else:
                    fileToParse_Intra = os.listdir(working_path + "/Intraday")
                    count = _parsing_and_writing_cdf(fileToParse_Intra,
                                                       working_path + "/Intraday",
                                                       outpath,
                                                       is_daily=False,
                                                       yyyyymmdd=yyyymmdd,
                                                       processed_earliest_date=self.process_earliest_intra)
                success = count >= 1500
            except Exception as e:
                print(f"Error extracting or parsing: {e}")
            finally:
                shutil.rmtree(working_path, ignore_errors=True)
                pass
        except Exception as e:
            return (False, str(e))
        return (True, 'Processed: ' + str(count))


    def run(self, ctx: PipelineContext) -> Tuple[StageResult, str]:
        errors: str = None
        try:
            workingfolder = ctx.params.get(self.kibot_process_section, 'workingfolder')
            cdf_path  = ctx.params.get(self.kibot_process_section, 'cdf_path')
            rundate   = datetime.strptime(
                                ctx.params.get(self.kibot_process_section, 'rundate'),
                         "%Y-%m-%d").date()
            daily = ctx.params.getboolean(self.kibot_process_section, 'daily')
            archive   = ctx.params.get(self.kibot_process_section, 'archive')

            archive_in = archive + '/' + rundate.strftime('%Y%m%d') + '.exe'

            success, message = self._process_kibot_archive(archive_in,
                                            workingfolder,
                                            cdf_path,
                                            daily)

            if success:
                return (StageResult.SUCCESS, message)
            else:
                return (StageResult.FAIL, message)

        except Exception as e:
            errors = 'Error while running KibotDailyFtpDump ' + self.name + ': ' + str(e)
            ctx.logger.error(errors)
            return (StageResult.FAIL, message)

